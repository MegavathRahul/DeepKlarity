{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27Uu_tVeDVgj",
        "outputId": "9a331f3f-92fb-48a6-876a-fb2feef7d644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "The folder you are executing pip from can no longer be found.\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import requests\n"
      ],
      "metadata": {
        "id": "ogdfn0MzDou7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "3Z26YFLtFmiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_bbc():\n",
        "    url = \"https://www.bbc.com/news\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = []\n",
        "\n",
        "    for item in soup.find_all('h3', class_='gs-c-promo-heading__title'):\n",
        "        title = item.get_text()\n",
        "        link = item.find_parent('a')['href']\n",
        "        if not link.startswith('http'):\n",
        "            link = 'https://www.bbc.com' + link\n",
        "        articles.append({\n",
        "            'Title': title,\n",
        "            'Source': 'BBC',\n",
        "            'URL': link,\n",
        "            'Publication Date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            'Summary': ''\n",
        "        })\n",
        "    return articles"
      ],
      "metadata": {
        "id": "MVnFlsmXF1-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_cnn():\n",
        "    url = \"https://www.cnn.com\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = []\n",
        "\n",
        "    for item in soup.find_all('h3', class_='cd__headline'):\n",
        "        title = item.get_text()\n",
        "        link = item.find_parent('a')['href']\n",
        "        if not link.startswith('http'):\n",
        "            link = 'https://www.cnn.com' + link\n",
        "        articles.append({\n",
        "            'Title': title,\n",
        "            'Source': 'CNN',\n",
        "            'URL': link,\n",
        "            'Publication Date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            'Summary': ''\n",
        "        })\n",
        "    return articles"
      ],
      "metadata": {
        "id": "MmbfinLUF5Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(articles):\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('news_articles.csv', index=False)"
      ],
      "metadata": {
        "id": "N84_HfdhGArI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_articles = scrape_bbc()\n",
        "cnn_articles = scrape_cnn()\n",
        "all_articles = bbc_articles + cnn_articles\n",
        "save_to_csv(all_articles)"
      ],
      "metadata": {
        "id": "xxf5mIYJGG-k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "516d62c7-89b1-4f57-a33b-1f842bf788b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'news_articles.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-c83bda0db66b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcnn_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbc_articles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcnn_articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msave_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-79-abdc5096788e>\u001b[0m in \u001b[0;36msave_to_csv\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'news_articles.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'news_articles.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('news_articles.csv')\n"
      ],
      "metadata": {
        "id": "3Y5XJub7HDgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_bbc():\n",
        "    url = \"https://www.bbc.com/news\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = []\n",
        "\n",
        "    for item in soup.select('h3 a'):\n",
        "        title = item.get_text()\n",
        "        link = item['href']\n",
        "        if not link.startswith('http'):\n",
        "            link = 'https://www.bbc.com' + link\n",
        "        articles.append({\n",
        "            'Title': title,\n",
        "            'Source': 'BBC',\n",
        "            'URL': link,\n",
        "            'Publication Date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            'Summary': ''\n",
        "        })\n",
        "    return articles\n"
      ],
      "metadata": {
        "id": "YM_xL8YtITkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_cnn():\n",
        "    url = \"https://www.cnn.com\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = []\n",
        "\n",
        "    for item in soup.select('h3.cd__headline a'):\n",
        "        title = item.get_text()\n",
        "        link = item['href']\n",
        "        if not link.startswith('http'):\n",
        "            link = 'https://www.cnn.com' + link\n",
        "        articles.append({\n",
        "            'Title': title,\n",
        "            'Source': 'CNN',\n",
        "            'URL': link,\n",
        "            'Publication Date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            'Summary': ''\n",
        "        })\n",
        "    return articles\n"
      ],
      "metadata": {
        "id": "J-ddqk5_KYSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(articles):\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('news_articles.csv', index=False)"
      ],
      "metadata": {
        "id": "63XRUffjKaMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_articles = scrape_bbc()\n",
        "cnn_articles = scrape_cnn()\n",
        "all_articles = bbc_articles + cnn_articles\n",
        "save_to_csv(all_articles)"
      ],
      "metadata": {
        "id": "qHi5MPmRKv5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('news_articles.csv')"
      ],
      "metadata": {
        "id": "9AKBxVtLKzaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_bbc():\n",
        "    url = \"https://www.bbc.com/news\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = []\n",
        "\n",
        "    for item in soup.find_all('h2', class_='sc-1207bea1-3 cZBSCm'):\n",
        "        title = item.get_text()\n",
        "        link = item.find_parent('a')['href']\n",
        "        if not link.startswith('http'):\n",
        "            link = 'https://www.bbc.com' + link\n",
        "\n",
        "        article_response = requests.get(link)\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "        summary = article_soup.find('meta', {'name': 'description'})\n",
        "        summary_text = summary['content'] if summary else 'No summary available'\n",
        "\n",
        "        articles.append({\n",
        "            'Title': title,\n",
        "            'Source': 'BBC',\n",
        "            'URL': link,\n",
        "            'Publication Date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            'Summary': summary_text\n",
        "        })\n",
        "    return articles"
      ],
      "metadata": {
        "id": "ltOhL2DGNHpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(articles):\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('news_articles.csv', index=False)"
      ],
      "metadata": {
        "id": "zq5GixgdUeqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('news_articles.csv')"
      ],
      "metadata": {
        "id": "Mx4AeM4nUmay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_cnn():\n",
        "    url = \"https://www.cnn.com/news\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = []\n",
        "\n",
        "    for item in soup.find_all('h2', class_='sc-1207bea1-3 cZBSCm'):\n",
        "        title = item.get_text()\n",
        "        link = item.find_parent('a')['href']\n",
        "        if not link.startswith('http'):\n",
        "            link = 'https://www.cnn.com' + link\n",
        "\n",
        "        article_response = requests.get(link)\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "        summary = article_soup.find('meta', {'name': 'description'})\n",
        "        summary_text = summary['content'] if summary else 'No summary available'\n",
        "\n",
        "        articles.append({\n",
        "            'Title': title,\n",
        "            'Source': 'CNN',\n",
        "            'URL': link,\n",
        "            'Publication Date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            'Summary': summary_text\n",
        "        })\n",
        "    return articles"
      ],
      "metadata": {
        "id": "FeTA4EsTNkSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(articles):\n",
        "    df = pd.DataFrame(articles)\n",
        "    df.to_csv('news_articles.csv', index=False)"
      ],
      "metadata": {
        "id": "gBPzTqnbNscd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_articles = scrape_cnn()\n",
        "save_to_csv(cnn_articles)"
      ],
      "metadata": {
        "id": "wdsWtsG_VGAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_articles = scrape_cnn()\n",
        "save_to_csv(bbc_articles)"
      ],
      "metadata": {
        "id": "iDIVYLUej4Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_articles = scrape_bbc()\n",
        "cnn_articles = scrape_cnn()\n",
        "all_articles = bbc_articles + cnn_articles\n",
        "save_to_csv(all_articles)"
      ],
      "metadata": {
        "id": "6ja53UTENuJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('news_articles.csv')"
      ],
      "metadata": {
        "id": "r-3xpMH7Ny60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_bbc():\n",
        "    url = \"https://www.bbc.com/news\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = []\n",
        "\n",
        "    for item in soup.find_all('h2', class_='sc-1207bea1-3 cZBSCm'):\n",
        "        title = item.get_text()\n",
        "        link = item.find_parent('a')['href']\n",
        "        if not link.startswith('http'):\n",
        "            link = 'https://www.bbc.com' + link\n",
        "\n",
        "        article_response = requests.get(link)\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "        summary = article_soup.find('meta', {'name': 'description'})\n",
        "        summary_text = summary['content'] if summary else 'No summary available'\n",
        "\n",
        "        articles.append({\n",
        "            'Title': title,\n",
        "            'Source': 'BBC',\n",
        "            'URL': link,\n",
        "            'Publication Date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            'Summary': summary_text\n",
        "        })\n",
        "    return articles\n",
        "\n",
        "def scrape_cnn():\n",
        "    url = \"https://www.cnn.com/news\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = []\n",
        "\n",
        "    for item in soup.find_all('h2', class_='sc-1207bea1-3 cZBSCm'):\n",
        "        title = item.get_text()\n",
        "        link = item.find_parent('a')['href']\n",
        "        if not link.startswith('http'):\n",
        "            link = 'https://www.cnn.com' + link\n",
        "\n",
        "        article_response = requests.get(link)\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "        summary = article_soup.find('meta', {'name': 'description'})\n",
        "        summary_text = summary['content'] if summary else 'No summary available'\n",
        "\n",
        "        articles.append({\n",
        "            'Title': title,\n",
        "            'Source': 'CNN',\n",
        "            'URL': link,\n",
        "            'Publication Date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            'Summary': summary_text\n",
        "        })\n",
        "    return articles"
      ],
      "metadata": {
        "id": "triprunDo7jY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}